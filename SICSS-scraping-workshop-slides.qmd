---
title: "Web Scraping en R"
subtitle: "SICSS - Montr√©al 2023"
author: Alex Luscombe
date: "2023-06-14"
format: 
  revealjs:
    incremental: false
    highlight-style: github
    toc: true
    toc-depth: 1
    theme: simple
    slide-number: true
    self-contained: true
editor: visual
lang: fr
---

# Comprendre le Web Scraping

## Comprendre le Web Scraping

-   Web Scraping : le processus automatis√© d'extraction de donn√©es √† partir de sites web

-   En `R`, nous utilisons couramment les biblioth√®ques `rvest` et `RSelenium` pour accomplir cela

-   Le web scraping pour la recherche en sciences sociales ouvre de nombreuses opportunit√©s, mais il pr√©sente √©galement de nombreux d√©fis

## D√©fis et opportunit√©s du Web Scraping

-   **Opportunit√©s** :

    -   Des donn√©es nouvelles et accessibles

    -   Informations en temps r√©el

    -   Recherche √† m√©thodes mixtes

    -   Analyse comparative

-   **D√©fis** :

    -   Consid√©rations √©thiques et l√©gales

    -   Complexit√© technique

    -   Qualit√© des donn√©es et biais

## Consid√©rations l√©gales et √©thiques

-   *Robots.txt* : Respectez les directives sp√©cifi√©es dans le fichier robots.txt du site web pour les autorisations et les restrictions de scraping

-   User-agents et en-t√™tes : Personnalisez les user-agents et les en-t√™tes pour fournir des informations d'identification pertinentes dans les requ√™tes de scraping

-   Conditions d'utilisation : Examinez et respectez les conditions d'utilisation du site web qui est scrap√©

## Consid√©rations l√©gales et √©thiques

-   Restrictions d'utilisation des donn√©es : Comprenez et respectez les √©ventuelles limitations ou restrictions sur l'utilisation des donn√©es extraites

-   √âthique : Tenez compte des pr√©occupations li√©es √† la vie priv√©e, de l'impact sur le site web ou la source de donn√©es, et respectez les bonnes pratiques de scraping √©thique

## /robots.txt

-   *Robots.txt* : un fichier texte situ√© √† la racine d'un site web qui fournit des instructions aux robots d'exploration web, y compris les bots de scraping, sur ce qui peut et ne peut pas √™tre acc√©d√©

-   Identifier le fichier robots.txt : Recherchez le fichier robots.txt en ajoutant \<\< /robots.txt \>\> √† l'URL du site web

## Jargon du fichier /robots.txt

-   `User-agent` : Sp√©cifie le robot d'exploration ou user-agent auquel les r√®gles s'appliquent

    -   \<\< \* \>\> (wildcard) : S'applique √† tous les robots d'exploration web

    -   \<\< User-agent: \[user-agent sp√©cifique\] \>\> : Concerne un robot d'exploration web ou bot de scraping sp√©cifique

## Jargon du fichier /robots.txt

-   `Disallow` : Liste les r√©pertoires ou fichiers qui ne doivent pas √™tre explor√©s ou scrap√©s

-   `Allow` : Sp√©cifie les exceptions aux r√®gles de d√©sactivation, autorisant des r√©pertoires ou fichiers sp√©cifiques √† √™tre explor√©s ou scrap√©s

-   `Crawl-delay` : Respectez le d√©lai sp√©cifi√© pour √©viter de surcharger le serveur avec trop de requ√™tes en un laps de temps court

## Un exemple de fichier robots.txt

``` txt
User-agent: Bingbot
Disallow: /

User-agent: *
Crawl-delay: 5
Disallow: /private/
Disallow: /admin/
Disallow: /confidential/ 
Allow: /public/
```

-   Exemple concret : <https://www.utoronto.ca/robots.txt>

-   Noter que tous les sites web ne disposent pas d'un fichier robots.txt : <https://sicss.io/robots.txt>

# Bases de HTML

## Bases de HTML

`HTML` : HyperText Markup Language

-   Langage de balisage utilis√© pour structurer et pr√©senter le contenu sur le web

-   Les balises d√©finissent les √©l√©ments HTML, tels que les titres, les paragraphes, les listes, etc.

-   Les √©l√©ments peuvent √™tre imbriqu√©s les uns √† l'int√©rieur des autres pour cr√©er une structure hi√©rarchique

    ``` html
    <div class="container">
      <h1 id="titre-presentation"> Bienvenue dans ma pr√©sentation </h1>
      <p> Ceci est un exemple de texte en fran√ßais </p>
    </div>
    ```

## Structure d'un document HTML

-   `<html>` : √âl√©ment racine d'une page HTML

-   `<head>` : Contient les m√©tadonn√©es du document

-   `<body>` : Repr√©sente le contenu de la page HTML

## Inspection de la structure HTML

-   Utilisez les outils de d√©veloppeur du navigateur pour inspecter et analyser la structure HTML des pages web

-   Acc√©der les outils de d√©veloppeur : clic droit sur une page web, s√©lectionnez \<\< Inspect \>\> pour les ouvrir

    -   MacOS : `‚åò + Option + I`

    -   PC : `Ctrl + Shift + I`

# S√©lection et extraction de donn√©es

## Les s√©lecteurs CSS et XPath

-   Les s√©lecteurs `CSS` permettent de cibler des √©l√©ments HTML sp√©cifiques

    ``` html
    <div class="container">
      <h1 id="titre-presentation"> Bienvenue dans ma pr√©sentation </h1>
      <p> Ceci est un exemple de texte en fran√ßais </p>
    </div>
    ```

-   `XPath` est un langage d'expression pour naviguer dans les documents `XML`

    ``` xpath
    //div[@class='container']/h1
    ```

-   Outil utile : [SelectorGadget](https://selectorgadget.com/)

## Extraction de donn√©es (GET request)

![[Source de l'image](https://bytesofgigabytes.com/networking/how-http-request-and-response-works/)](assets/http-communication.png){fig-align="left"}

## S√©lection et extraction avec rvest

``` r
library(rvest)

# Sp√©cifiez l'URL de la page web
url <- "https://www.example.com"

# Effectuez une requ√™te pour obtenir le contenu HTML de la page web 
# puis lisez-le - rvest::read_html le fait tout en un
webpage <- read_html(url)

# Extrait le contenu de l'√©l√©ment HTML avec l'id="titre-presentation"
title_content <- webpage |> 
  html_nodes("#titre-presentation") |>
  html_text()

# Extrait le contenu de l'√©l√©ment HTML avec la classe="container"
container_content <- webpage |> 
  html_nodes(".container") |>
  html_text()
```

# Contenu dynamique

## Contenu dynamique

-   Les m√©thodes traditionnelles de scraping peuvent ne pas capturer efficacement le contenu dynamique

-   Le contenu dynamique fait r√©f√©rence aux √©l√©ments des sites web qui changent ou se chargent dynamiquement √† l'aide de `JavaScript`

-   Les sites web dynamiques reposent sur le rendu c√¥t√© client, ce qui rend l'extraction des donn√©es difficile

-   Exemple : <https://sicss.io/people>

## Contenu dynamique

-   Deux solutions :

    -   `API` : Explorez les APIs disponibles qui offrent un acc√®s direct aux donn√©es dynamiques, n√©cessitant g√©n√©ralement cl√© d'API

        -   Parfois, les APIs sont c√¥t√© client, ce qui signifie qu'elles sont ex√©cut√©es dans le navigateur du client plut√¥t que sur le serveur

    -   `RSelenium`\* : Paquet R avec des liens Selenium pour l'automatisation du web et le scraping de contenu dynamique (\**Cela fonctionne mieux en Python* üêç)

# Le Web Scraping bas√© sur le Cloud (‚ö†Ô∏è avanc√©!)

## Le Web Scraping bas√© sur le Cloud

-   Le Web Scraping bas√© sur le Cloud permet aux chercheurs d'automatiser et de collecter continuellement des informations

-   Des exemples d'outils de Web Scraping bas√©s sur le Cloud incluent GitHub Actions, AWS et Heroku

-   Chacun de ces outils peut g√©rer l'ex√©cution de scripts R c√¥t√© serveur, permettant aux chercheurs d'utiliser ces plateformes pour ex√©cuter des t√¢ches de Web Scraping

-   Tutoriel : <https://www.r-bloggers.com/2021/03/daily-stock-gainers-automated-web-scraping-in-r-with-github-actions/>

# Questions et r√©ponses

# Pause de 5 minutes

# Tutoriel
